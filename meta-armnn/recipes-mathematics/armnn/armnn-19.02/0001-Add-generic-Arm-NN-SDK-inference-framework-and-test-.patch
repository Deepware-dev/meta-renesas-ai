From 2509575fef1db6a2d25061906b727cfd459edfc0 Mon Sep 17 00:00:00 2001
From: Jianming Qiao <jianming.qiao@bp.renesas.com>
Date: Mon, 19 Aug 2019 21:44:41 +0100
Subject: [PATCH 1/1] Add generic Arm NN SDK inference framework and test code

A C++ framework and test that is able to seperate the original
Armnn test framework class into following four independent steps:
1. Image processing
2. Model loading
3. Inferencing
4. Postprocessing of the prediction results

Signed-off-by: Jianming Qiao <jianming.qiao@bp.renesas.com>
---
 tests/CMakeLists.txt                              |  29 ++
 tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp | 433 ++++++++++++++++++++++
 2 files changed, 462 insertions(+)
 create mode 100644 tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp

diff --git a/tests/CMakeLists.txt b/tests/CMakeLists.txt
index 9913321..9f9aec6 100644
--- a/tests/CMakeLists.txt
+++ b/tests/CMakeLists.txt
@@ -245,6 +245,18 @@ if (BUILD_ARMNN_SERIALIZER OR BUILD_CAFFE_PARSER OR BUILD_TF_PARSER OR BUILD_TF_
     set(ExecuteNetwork_sources
         ExecuteNetwork/ExecuteNetwork.cpp)
 
+    set(Renesas-Reference-Code-Armnn_sources
+        RenesasSample-Armnn/RenesasSample-Armnn.cpp
+        ImagePreprocessor.hpp
+        ImagePreprocessor.cpp
+        InferenceTestImage.hpp
+        InferenceTestImage.cpp)
+
+    add_executable_ex(RenesasSample-Armnn ${Renesas-Reference-Code-Armnn_sources})
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/armnn)
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/armnnUtils)
+    target_include_directories(RenesasSample-Armnn PRIVATE ../src/backends)
+
     add_executable_ex(ExecuteNetwork ${ExecuteNetwork_sources})
     target_include_directories(ExecuteNetwork PRIVATE ../src/armnn)
     target_include_directories(ExecuteNetwork PRIVATE ../src/armnnUtils)
@@ -252,29 +264,46 @@ if (BUILD_ARMNN_SERIALIZER OR BUILD_CAFFE_PARSER OR BUILD_TF_PARSER OR BUILD_TF_
 
     if (BUILD_ARMNN_SERIALIZER)
         target_link_libraries(ExecuteNetwork armnnSerializer)
+        target_link_libraries(RenesasSample-Armnn armnnSerializer)
     endif()
     if (BUILD_CAFFE_PARSER)
         target_link_libraries(ExecuteNetwork armnnCaffeParser)
+        target_link_libraries(RenesasSample-Armnn armnnCaffeParser)
     endif()
     if (BUILD_TF_PARSER)
         target_link_libraries(ExecuteNetwork armnnTfParser)
+        target_link_libraries(RenesasSample-Armnn armnnTfParser)
     endif()
 
     if (BUILD_TF_LITE_PARSER)
         target_link_libraries(ExecuteNetwork armnnTfLiteParser)
+        target_link_libraries(RenesasSample-Armnn armnnTfLiteParser)
     endif()
     if (BUILD_ONNX_PARSER)
             target_link_libraries(ExecuteNetwork armnnOnnxParser)
+            target_link_libraries(RenesasSample-Armnn armnnOnnxParser)
     endif()
 
     target_link_libraries(ExecuteNetwork armnn)
     target_link_libraries(ExecuteNetwork ${CMAKE_THREAD_LIBS_INIT})
+
+    target_link_libraries(RenesasSample-Armnn armnn)
+    target_link_libraries(RenesasSample-Armnn ${CMAKE_THREAD_LIBS_INIT})
+
     if(OPENCL_LIBRARIES)
         target_link_libraries(ExecuteNetwork ${OPENCL_LIBRARIES})
+        target_link_libraries(RenesasSample-Armnn ${OPENCL_LIBRARIES})
     endif()
     target_link_libraries(ExecuteNetwork
         ${Boost_SYSTEM_LIBRARY}
         ${Boost_FILESYSTEM_LIBRARY}
         ${Boost_PROGRAM_OPTIONS_LIBRARY})
     addDllCopyCommands(ExecuteNetwork)
+
+    target_link_libraries(RenesasSample-Armnn
+        ${Boost_SYSTEM_LIBRARY}
+        ${Boost_FILESYSTEM_LIBRARY}
+        ${Boost_PROGRAM_OPTIONS_LIBRARY})
+    addDllCopyCommands(RenesasSample-Armnn)
+
 endif()
diff --git a/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp b/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp
new file mode 100644
index 0000000..00327b8
--- /dev/null
+++ b/tests/RenesasSample-Armnn/RenesasSample-Armnn.cpp
@@ -0,0 +1,433 @@
+ï»¿/* 
+ * Copyright (C) 2019 Renesas Electronics Corp. 
+ * This file is licensed under the terms of the MIT License
+ * This program is licensed "as is" without any warranty of any
+ * kind, whether express or implied.
+ */
+
+#include <armnn/ArmNN.hpp>
+#include <armnn/TypesUtils.hpp>
+
+#if defined(ARMNN_CAFFE_PARSER)
+#include "armnnCaffeParser/ICaffeParser.hpp"
+#endif
+#if defined(ARMNN_TF_PARSER)
+#include "armnnTfParser/ITfParser.hpp"
+#endif
+#if defined(ARMNN_TF_LITE_PARSER)
+#include "armnnTfLiteParser/ITfLiteParser.hpp"
+#endif
+#if defined(ARMNN_ONNX_PARSER)
+#include "armnnOnnxParser/IOnnxParser.hpp"
+#endif
+
+#include "CsvReader.hpp"
+#include "../InferenceTest.hpp"
+
+#include <Logging.hpp>
+#include <Profiling.hpp>
+#include "../ImagePreprocessor.hpp"
+#include "../InferenceTestImage.hpp"
+
+#include <boost/algorithm/string/trim.hpp>
+#include <boost/algorithm/string/split.hpp>
+#include <boost/algorithm/string/classification.hpp>
+#include <boost/program_options.hpp>
+#include <boost/variant.hpp>
+
+#include <iostream>
+#include <fstream>
+#include <functional>
+#include <future>
+#include <algorithm>
+#include <iterator>
+#include <numeric>
+
+#define NUMBER_RUN_TESTS 30
+
+std::map<int,std::string> label_file_map;
+
+string base_path = "/usr/bin/armnn/examples";
+
+typedef struct model_params {
+    std::string modelFormat;
+    bool isFloatModel;
+    bool bNeedSoftMax;
+    std::string modelPath;
+    armnn::TensorShape inputTensorShape;
+    std::string inputName;
+    std::string outputName;
+    unsigned int inputImageWidth;
+    unsigned int inputImageHeight;
+}model_params;
+
+std::map<std::string,model_params> Model_Table;
+
+template <typename TDataType>
+int ProcessResult(std::vector<TDataType>& output,InferenceModelInternal::QuantizationParams quantParams,const string mode_type)
+{
+    std::map<float,int> resultMap;
+
+    int index = 0;
+    
+    for (const auto & o : output)
+    {
+        float prob = ToFloat<TDataType>::Convert(o, quantParams);
+        int classification = index++;
+
+        std::map<float, int>::iterator lb = resultMap.lower_bound(prob);
+        if (lb == resultMap.end() ||
+            !resultMap.key_comp()(prob, lb->first)) {
+            resultMap.insert(lb, std::map<float, int>::value_type(prob, classification));
+        }
+    }
+
+    BOOST_LOG_TRIVIAL(info) << "= Prediction values for test ";
+
+    auto it = resultMap.rbegin();
+    for (int i=0; i<5 && it != resultMap.rend(); ++i)
+    {
+        BOOST_LOG_TRIVIAL(info) << "Top(" << (i+1) << ") prediction is " << it->second <<
+            " with confidence: " << 100.0*(it->first) << "%";
+
+        if(mode_type == "onnx")
+            std::cout << "Result is " << label_file_map[it->second+1] << std::endl;
+        else
+            std::cout << "Result is " << label_file_map[it->second] << std::endl;
+
+        ++it;
+    }
+
+    return 0;
+}
+
+void CaculateAvergeDeviation(vector<double>& time_vec)
+{
+    double sum = std::accumulate(time_vec.begin(), time_vec.end(), 0.0);
+    double mean = sum / static_cast<double>(time_vec.size());
+
+    std::vector<double> diff(time_vec.size());
+    std::transform(time_vec.begin(), time_vec.end(), diff.begin(),
+                   std::bind2nd(std::minus<double>(), mean));
+    double sq_sum = std::inner_product(diff.begin(), diff.end(), diff.begin(), 0.0);
+    double stdev = std::sqrt(sq_sum / static_cast<double>(time_vec.size()));
+          
+    std::cout << "Total Time Takes " << (sum) << " ms"<< std::endl;
+
+    std::cout << "Average Time Takes " << (mean) << " ms"<< std::endl;
+
+    std::cout << "Standard Deviation " << stdev << std::endl;
+}
+
+template<typename TParser, typename TDataType>
+int MainImpl(const char* modelPath,
+             bool isFloatModel,
+             bool bNeedSoftMax,
+             const string mode_type,
+             const char* inputName,
+             const armnn::TensorShape* inputTensorShape,
+             const char* inputTensorDataFilePath,
+             const string inputImageName,
+             const unsigned int inputImageWidth,
+             const unsigned int inputImageHeight,
+             const char* outputName,
+             bool enableProfiling,
+             const size_t subgraphId,
+             const std::shared_ptr<armnn::IRuntime>& runtime = nullptr)
+{
+    // Loads input tensor.
+    std::vector<TDataType> input;
+    
+    std::ifstream inputTensorFile(inputTensorDataFilePath);
+    if (!inputTensorFile.good())
+    {
+        BOOST_LOG_TRIVIAL(fatal) << "Failed to load input tensor data file from " << inputTensorDataFilePath;
+        return EXIT_FAILURE;
+    }
+
+    using TContainer = boost::variant<std::vector<float>, std::vector<int>, std::vector<unsigned char>>;
+
+    std::vector<TContainer> inputDataContainers;
+
+    std::vector<TContainer> outputDataContainers;
+
+    std::vector<ImageSet> imageSet =
+    {
+        {inputImageName, 0},
+    };
+    
+    try
+    {
+        // Creates an InferenceModel, which will parse the model and load it into an IRuntime.
+        typename InferenceModel<TParser, TDataType>::Params params;
+        params.m_ModelPath = modelPath;
+        params.m_IsModelBinary = true;
+        params.m_InputBindings.push_back(std::string(inputName));
+        params.m_InputShapes.push_back(*inputTensorShape);
+        params.m_OutputBindings.push_back(outputName);
+        params.m_EnableProfiling = enableProfiling;
+        params.m_SubgraphId = subgraphId;
+        params.m_ComputeDevices = {armnn::Compute::CpuAcc};
+        InferenceModel<TParser, TDataType> model(params, runtime);
+
+        // Executes the model.
+        std::unique_ptr<ClassifierTestCaseData<TDataType>> TestCaseData;
+
+        if(isFloatModel)
+        {
+            if(mode_type == "onnx")
+            {
+                ImagePreprocessor<TDataType>  Image(inputTensorDataFilePath,inputImageWidth,inputImageHeight,imageSet,\
+                                                    1.0,0,{{0.485f, 0.456f, 0.406f}},{{0.229f, 0.224f, 0.225f}},\
+                                                    ImagePreprocessor<TDataType>::DataFormat::NCHW);
+
+                TestCaseData = Image.GetTestCaseData(0);
+            }
+            else
+            {
+                ImagePreprocessor<TDataType> Image(inputTensorDataFilePath,inputImageWidth,inputImageHeight,imageSet);
+                TestCaseData = Image.GetTestCaseData(0);
+            }
+
+            outputDataContainers.push_back(std::vector<float>(model.GetOutputSize()));
+        }
+        else
+        {
+            std::cout << "Quant Model is loaded" << std::endl;
+            auto inputBinding = model.GetInputBindingInfo();
+            printf("Scale %f\n",inputBinding.second.GetQuantizationScale());
+            printf("Offset %d\n",inputBinding.second.GetQuantizationOffset());
+            ImagePreprocessor<TDataType>  Image(inputTensorDataFilePath,inputImageWidth,inputImageHeight,imageSet,\
+	   					inputBinding.second.GetQuantizationScale(), \
+						inputBinding.second.GetQuantizationOffset());
+
+            TestCaseData = Image.GetTestCaseData(0);
+            
+            outputDataContainers.push_back(std::vector<uint8_t>(model.GetOutputSize()));
+
+        }
+
+        inputDataContainers.push_back(TestCaseData->m_InputImage);
+
+        //warm up
+        model.Run(inputDataContainers, outputDataContainers);
+     
+        time_point<high_resolution_clock> predictStart;
+        time_point<high_resolution_clock> predictEnd;
+
+        std::vector<double> time_vector;
+
+        for(unsigned int i = 0; i < NUMBER_RUN_TESTS; i++)
+        {
+            predictStart = high_resolution_clock::now();
+
+            model.Run(inputDataContainers, outputDataContainers);
+
+            predictEnd = high_resolution_clock::now();
+
+            double timeTakenS = duration<double>(predictEnd - predictStart).count();
+
+            time_vector.push_back(timeTakenS*1000.0);
+        }
+
+        CaculateAvergeDeviation(time_vector);
+
+        if(isFloatModel)
+        {
+            std::vector<float> output;
+            output = boost::get<std::vector<float>>(outputDataContainers[0]);
+            ProcessResult<float>(output,model.GetQuantizationParams(),mode_type);
+        }
+        else
+        {
+            std::vector<unsigned char> output;
+            output = boost::get<std::vector<unsigned char>>(outputDataContainers[0]);
+            ProcessResult<unsigned char>(output,model.GetQuantizationParams(),mode_type);
+        }
+    }
+    catch (armnn::Exception const& e)
+    {
+        BOOST_LOG_TRIVIAL(fatal) << "Armnn Error: " << e.what();
+        return EXIT_FAILURE;
+    }
+
+    return EXIT_SUCCESS;
+}
+
+void initModelTable()
+{   
+   //Basic Model Verification
+    Model_Table["mobilenet_v1_1.0_224_frozen.pb"] = {"tensorflow-binary", true , false , base_path + "/tensorflow/models/mobilenet_v1_1.0_224_frozen.pb",armnn::TensorShape({ 1, 224, 224, 3}),"input","MobilenetV1/Predictions/Reshape_1", 224, 224};
+
+    Model_Table["mobilenet_v1_1.0_224_quant.tflite"] = {"tflite-binary", false , false ,base_path + "/tensorflow-lite/models/mobilenet_v1_1.0_224_quant.tflite",armnn::TensorShape({ 1, 224, 224, 3}),"input","MobilenetV1/Predictions/Reshape_1", 224, 224};
+}
+
+// This will run a test
+template<typename TDataType>
+int RunTest(const std::string& modelFormat,
+            const bool isFloatModel,
+            const bool bNeedSoftMax,
+            const armnn::TensorShape& inputTensorShape,
+            const std::string& modelPath,
+            const std::string& inputName,
+            const std::string& inputTensorDataFilePath,
+            const std::string& inputImageName,
+            const unsigned int inputImageWidth,
+            const unsigned int inputImageHeight,
+            const std::string& outputName,
+            bool enableProfiling,
+            const size_t subgraphId,
+            const std::shared_ptr<armnn::IRuntime>& runtime = nullptr)
+{
+    // Parse model binary flag from the model-format string we got from the command-line
+    bool isModelBinary;
+    if (modelFormat.find("bin") != std::string::npos)
+    {
+        isModelBinary = true;
+    }
+    else if (modelFormat.find("txt") != std::string::npos || modelFormat.find("text") != std::string::npos)
+    {
+        isModelBinary = false;
+    }
+    else
+    {
+        BOOST_LOG_TRIVIAL(fatal) << "Unknown model format: '" << modelFormat << "'. Please include 'binary' or 'text'";
+        return EXIT_FAILURE;
+    }
+
+    // Forward to implementation based on the parser type
+    if (modelFormat.find("caffe") != std::string::npos)
+    {
+#if defined(ARMNN_CAFFE_PARSER)
+        return MainImpl<armnnCaffeParser::ICaffeParser, TDataType>(modelPath.c_str(), isFloatModel,bNeedSoftMax,"caffe",
+                                                               inputName.c_str(), &inputTensorShape,
+                                                               inputTensorDataFilePath.c_str(), inputImageName, 
+                                                               inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                               enableProfiling, subgraphId, runtime);
+#else
+        BOOST_LOG_TRIVIAL(fatal) << "Not built with Caffe parser support.";
+        return EXIT_FAILURE;
+#endif
+    }
+    else if (modelFormat.find("onnx") != std::string::npos)
+    {
+#if defined(ARMNN_ONNX_PARSER)
+        return MainImpl<armnnOnnxParser::IOnnxParser, float>(modelPath.c_str(), isFloatModel, bNeedSoftMax, "onnx",
+                                                         inputName.c_str(), &inputTensorShape,
+                                                         inputTensorDataFilePath.c_str(), inputImageName, 
+                                                         inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                         enableProfiling, subgraphId, runtime);
+#else
+        BOOST_LOG_TRIVIAL(fatal) << "Not built with Onnx parser support.";
+        return EXIT_FAILURE;
+#endif
+    }
+    else if (modelFormat.find("tensorflow") != std::string::npos)
+    {
+#if defined(ARMNN_TF_PARSER)
+        return MainImpl<armnnTfParser::ITfParser, TDataType>(modelPath.c_str(), isFloatModel, bNeedSoftMax,"tensorflow",
+                                                         inputName.c_str(), &inputTensorShape,
+                                                         inputTensorDataFilePath.c_str(), inputImageName, 
+                                                         inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                         enableProfiling, subgraphId, runtime);
+#else
+        BOOST_LOG_TRIVIAL(fatal) << "Not built with Tensorflow parser support.";
+        return EXIT_FAILURE;
+#endif
+    }
+    else if(modelFormat.find("tflite") != std::string::npos)
+    {
+#if defined(ARMNN_TF_LITE_PARSER)
+        if (! isModelBinary)
+        {
+            BOOST_LOG_TRIVIAL(fatal) << "Unknown model format: '" << modelFormat << "'. Only 'binary' format supported \
+              for tflite files";
+            return EXIT_FAILURE;
+        }
+        return MainImpl<armnnTfLiteParser::ITfLiteParser, TDataType>(modelPath.c_str(), isFloatModel,bNeedSoftMax, "tflite",
+                                                                 inputName.c_str(), &inputTensorShape,
+                                                                 inputTensorDataFilePath.c_str(), inputImageName, 
+                                                                 inputImageWidth, inputImageHeight, outputName.c_str(),
+                                                                 enableProfiling, subgraphId, runtime);
+#else
+        BOOST_LOG_TRIVIAL(fatal) << "Unknown model format: '" << modelFormat <<
+            "'. Please include 'caffe', 'tensorflow', 'tflite' or 'onnx'";
+        return EXIT_FAILURE;
+#endif
+    }
+    else
+    {
+        BOOST_LOG_TRIVIAL(fatal) << "Unknown model format: '" << modelFormat <<
+                                 "'. Please include 'caffe', 'tensorflow', 'tflite' or 'onnx'";
+        return EXIT_FAILURE;
+    }
+}
+
+void loadLabelFile(string label_file_name)
+{
+    std::ifstream infile(label_file_name);
+ 
+    string line;
+    while(std::getline(infile,line))
+    {
+        stringstream line_stream(line);
+        string item;
+        std::vector<string> item_vector;
+        while(std::getline(line_stream,item,':'))
+        {
+            //std::cout << item << std::endl;
+            item_vector.push_back(item);
+        }
+
+        label_file_map[std::stoi(item_vector[0])] = item_vector[1]; 
+    }
+}
+
+int main(int argc, const char* argv[])
+{
+    // Configures logging for both the ARMNN library and this test program.
+#ifdef NDEBUG
+    armnn::LogSeverity level = armnn::LogSeverity::Info;
+#else
+    armnn::LogSeverity level = armnn::LogSeverity::Debug;
+#endif
+    armnn::ConfigureLogging(true, true, level);
+    armnnUtils::ConfigureLogging(boost::log::core::get().get(), true, true, level);
+
+    initModelTable();
+
+    model_params params; 
+    
+    bool enableProfiling = false;
+    size_t subgraphId = 0;
+    string inputImageName = "grace_hopper.jpg";
+    string inputImagePath = "/usr/bin/armnn/examples/images/";
+
+    for ( auto it = Model_Table.begin(); it != Model_Table.end(); it++ )
+    {
+        params = Model_Table[it->first];    
+        
+        //load label file
+        string label_file_name = "/usr/bin/armnn/examples/tensorflow-lite/models/labels.txt";
+	
+        loadLabelFile(label_file_name);
+            
+        std::cout << "====================" << std::endl;
+        std::cout << "current model is " << it->first << std::endl;
+
+        if(params.isFloatModel)
+        {
+            RunTest<float>(params.modelFormat, params.isFloatModel, params.bNeedSoftMax, params.inputTensorShape, params.modelPath,\
+            params.inputName, inputImagePath, inputImageName,\
+            params.inputImageWidth,params.inputImageHeight, params.outputName, enableProfiling, subgraphId);
+        }
+        else
+        {
+            RunTest<uint8_t>(params.modelFormat, params.isFloatModel, params.bNeedSoftMax, params.inputTensorShape, params.modelPath,\
+            params.inputName, inputImagePath, inputImageName,\
+            params.inputImageWidth,params.inputImageHeight, params.outputName, enableProfiling, subgraphId);
+        }
+    }
+
+    return 0;
+}
-- 
2.7.4

